---
title: "Becas Crema 2"
author: "Cesar Jose Rodriguez Diaz"
date: "9 de marzo de 2016"
output: html_document
---

Primero instalamos y declaramos los paquetes necesarios

```{r, message=F, warning=F}
#install.packages('rpart')
#install.packages('rpart.plot')
#install.packages('caret')
#install.packages('class')
#install.packages('RWeka')
#install.packages('pROC')

library ('rpart')
library ('rpart.plot')
library('caret')
library('class')
library('RWeka')
library('pROC')
```

procedemos a cargar la data del archivo CSV

```{r}
dataSet <- read.csv2(
  file = "minable.csv",
  header = T,sep = ",")
```

Eliminamos un Outlier, la unica fila con metodo de ingreso 1

```{r}
dataSet <- dataSet[-c(1),]
```

Las columnas "factors" las volvemos numericas

```{r, warning=F}
for (i in 1:length(dataSet)){
  if (is.factor(dataSet[,i])){
    dataSet[,i] <- as.numeric(as.character(dataSet[,i]))
  }
}
```

Eliminamos las columnas que no nos interesan para el analisis

```{r, warning=F}
dataSet$cIdentidad <- NULL
dataSet$fNacimiento <- NULL
dataSet$sCurso <- NULL
dataSet$cDireccion <- NULL
dataSet$jReprobadas <- NULL
dataSet$mInscritas <- NULL
dataSet$tGrado <- NULL
dataSet$sCurso <- NULL
dataSet$pReside <- NULL
dataSet$dHabitacion <- NULL
dataSet$aEconomica <- NULL
dataSet$oSolicitudes <- NULL
dataSet$rEconomico <- NULL
dataSet$pRenovar <- NULL
dataSet$rating <- NULL
dataSet$sugerencias <- NULL
```

Sumamos los ingresos y gastos en columnas nuevas

```{r, warning=F}
dataSet$iTotal <- -1
dataSet$gTotal <- -1
dataSet$irTotal <- -1
dataSet$grTotal <- -1
for (i in 1:nrow(dataSet)){
  dataSet$iTotal[i] <- dataSet$beca[i] + dataSet$aResponsable[i] + dataSet$aOtros[i] + dataSet$iActividades
  dataSet$gTotal[i] <- dataSet$gAlimentacion[i] + dataSet$gTransporte[i] + dataSet$gMedicos[i] + dataSet$gOdontologicos[i] + dataSet$gPersonales[i] + dataSet$gEstudios[i] + dataSet$gRecreacion[i] + dataSet$gAlquiler[i] + dataSet$gOtros[i]
  dataSet$irTotal[i] <- dataSet$irMensual[i] + dataSet$irOtros[i]
  dataSet$grTotal[i] <- dataSet$grVivienda[i] + dataSet$grAlimentacion[i] + dataSet$grTransporte[i] + dataSet$grMedicos[i] + dataSet$grOdontologicos[i] + dataSet$grEducativos[i] + dataSet$grServicios[i] + dataSet$grCondominio[i] + dataSet$grOtros[i]
}
dataSet$grTotal[96] <- 15250
```

Eliminamos las columnas con el detalle de ingreso y gastos

```{r, warning=F}
dataSet$beca <- NULL
dataSet$aResponsable <- NULL
dataSet$aOtros <- NULL
dataSet$iActividades <- NULL
dataSet$gAlimentacion <- NULL
dataSet$gTransporte <- NULL
dataSet$gMedicos <- NULL
dataSet$gOdontologicos <- NULL
dataSet$gPersonales <- NULL
dataSet$gEstudios <- NULL
dataSet$gRecreacion <- NULL
dataSet$gAlquiler <- NULL
dataSet$gOtros <- NULL
dataSet$irMensual <- NULL
dataSet$irOtros <- NULL
dataSet$grVivienda <- NULL
dataSet$grAlimentacion <- NULL
dataSet$grTransporte <- NULL
dataSet$grMedicos <- NULL
dataSet$grOdontologicos <- NULL
dataSet$grEducativos <- NULL
dataSet$grServicios <- NULL
dataSet$grCondominio <- NULL
dataSet$grOtros <- NULL
```

Generamos las muestras estratificadas de entrenamiento y de prueba

```{r, warning=F}
set.seed(1886)
mIngreso0 = dataSet[dataSet$mIngreso==0,]
mIngreso1 = dataSet[dataSet$mIngreso==2,]
mIngreso2 = dataSet[dataSet$mIngreso==3,]
samp = sample(2,nrow(mIngreso0), replace = TRUE, prob = c(0.8,0.2))
samp1 = sample(2,nrow(mIngreso1), replace = TRUE, prob = c(0.8,0.2))
samp2 = sample(2,nrow(mIngreso2), replace = TRUE, prob = c(0.8,0.2))

t_entrenamiento = mIngreso0[samp==1,]
t_prueba = mIngreso0[samp==2,]
t_entrenamiento1 = mIngreso1[samp1==1,]
t_prueba1 = mIngreso1[samp1==2,]
t_entrenamiento2 = mIngreso2[samp2==1,]
t_prueba2 = mIngreso2[samp2==2,]

for (i in 1:nrow(t_entrenamiento1)){
  t_entrenamiento <- rbind(t_entrenamiento, t_entrenamiento1[i,])
}
for (i in 1:nrow(t_entrenamiento2)){
  t_entrenamiento <- rbind(t_entrenamiento, t_entrenamiento2[i,])
}
for (i in 1:nrow(t_prueba1)){
  t_prueba <- rbind(t_prueba, t_prueba1[i,])
}
for (i in 1:nrow(t_prueba2)){
  t_prueba <- rbind(t_prueba, t_prueba2[i,])
}
```

Generamos un arbol de decisión usando rpart con los parametros de "control" por defecto

```{r, warning=F}
tree <- rpart(mIngreso ~ ., data = t_entrenamiento, method = 'class')
confusionMatrixTree = table(t_prueba$mIngreso, predict(tree, newdata = t_prueba,type = "class"))
```

Iteranis los parametros de "control" de rpart en busca del mejor arbol posible y lo guardamos en la variable {best_tree}

```{r, warning=F}
minsplits <- c(2,5,10,50,300,1000)
cps <- c(0.3,0.2,0.1,0.0001,0.0000000000001)
minbuckets <- c(2,5,10,50,300)
best_Tree <- tree
min_error <- 1 - ((confusionMatrixTree[1,1] + confusionMatrixTree[2,2] + confusionMatrixTree[3,3]) / nrow(t_prueba))
for (i in minsplits){
  for(j in cps){
    for(k in minbuckets){
      tree <- rpart(mIngreso ~ ., t_entrenamiento,method = "class",control = rpart.control(minsplit = i,cp = j, minbucket = k))
      confusionMatrixTree = table(t_prueba$mIngreso, predict(tree, newdata = t_prueba,type = "class"))
      tree_error <- 1 - ((confusionMatrixTree[1,1] + confusionMatrixTree[2,2] + confusionMatrixTree[3,3]) / nrow(t_prueba))        
      if (tree_error < min_error){
        best_Tree <- tree
        min_error <- tree_error
      }
    }
  }
}
```

Mostramos el mejor arbol generado

```{r, warning=F}
rpart.plot(best_Tree)
```

Calculamos el error usando la tabla de prediccion

```{r, warning=F}
t_predict <- predict(best_Tree, newdata = t_prueba,type = "class")
confusionMatrixTree = table(t_prueba$mIngreso, predict(best_Tree, newdata = t_prueba,type = "class"))
tree_error <- 1 - ((confusionMatrixTree[1,1] + confusionMatrixTree[2,2] + confusionMatrixTree[3,3]) / nrow(t_prueba))        
tree_error
```

Normalizamos y generamos el modelo de predicción usando KNN

```{r, warning=F}
kv_entrenamiento <- t_entrenamiento
kv_prueba <- t_prueba
level_entrenamiento <- kv_entrenamiento$mIngreso
level_prueba <- kv_prueba$mIngreso
kv_entrenamiento$mIngreso <- NULL
kv_prueba$mIngreso <- NULL
normDatos = preProcess(kv_entrenamiento, method = "range")
kv_entrenamiento = predict(normDatos, kv_entrenamiento)
kv_prueba = predict(normDatos, kv_prueba)
kv_modelo <- knn(train = kv_entrenamiento, test = kv_prueba, cl = as.factor(level_entrenamiento), k=14)
```

Mosntramos el error del modelo con KNN

```{r, warning=F}
confusionMatrixKv = table(level_prueba, kv_modelo)
kv_error = 1 - ((confusionMatrixKv[1,1] + confusionMatrixKv[2,2] + confusionMatrixKv[3,3]) / nrow(t_prueba))
kv_error
```

Generamos el modelo de prediccion usando reglas de clasificación

```{r, warning=F}
t_entrenamiento$mIngreso <- as.factor(t_entrenamiento$mIngreso)
t_prueba$mIngreso <- as.factor(t_prueba$mIngreso)
rc_modelo <- JRip(formula = mIngreso ~ ., data = t_entrenamiento)
rc_predict <- predict(rc_modelo, newdata = t_prueba,type = "class")
```

Mostramos el error de este modelo

```{r, warning=F}
confusionMatrixClasification = table(t_prueba$mIngreso, rc_predict)
rc_error = 1 - ((confusionMatrixClasification[1,1] + confusionMatrixClasification[2,2] + confusionMatrixClasification[3,3]) / nrow(t_prueba))
rc_error
```

Analisis ROC del Arbol generado, tambien mostramos el AUC

```{r, warning=F}
rocT <- roc(t_prueba$mIngreso, as.numeric(as.character(t_predict)), levels=level_prueba)
plot(rocT)
```

Analisis ROC del modelo KNN, tambien mostramos el AUC

```{r, warning=F}
rocKv <- roc(t_prueba$mIngreso, as.numeric(as.character(kv_modelo)), levels=level_prueba)
plot(rocKv)
```

Analisis ROC de reglas de clasificación, tambien mostramos el AUC

```{r, warning=F}
rocRc <- roc(t_prueba$mIngreso, as.numeric(as.character(rc_predict)), levels=level_prueba)
plot(rocRc)
```



Al terminar todos los analisis podemos concluir de que el mejor modelo es el de reglas de clasificación, porque a pesar que el analisis ROC no nos muestra una ventaja en ningun modelo, el error de generado por este modelo es de 20%, tan bajo como el del Arbol pero se puede generar con mejor esfuerzo. 